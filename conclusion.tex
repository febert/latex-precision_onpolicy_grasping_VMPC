\section{Discussion}

In both \cite{hermans2013learning,salganicoff1993vision} pushing of unknown objects is learned from interaction data between the robot and objects. However the models employed in these works rely on hand-engineered input features (e.g. in the form of the approach angle to the object \cite{salganicoff1993vision} or a set of points on the silhouette of an object \cite{hermans2013learning}) which can make it hard to scale to complex real-world scenarios where for example objects can be partially occluded or cluttered. 
It as been shown in \cite{goldfeder2009data} and others that grasps on unseen objects can be found by matching the sensed 3D geometry to a database of precomputed object-grasp pairs. One downside to this approach is that physics are generally ignored in this scheme. For example information about mass and friction properties which can be estimated from visual clues (but not from geometry alone) cannot be leveraged. In \cite{mahler2017dex} large databases of 3D meshes are used to label synthetic depth images with an analytic physics based grasp quality metric. This approach has the downside of relying on a suitable metric which is not available for other manipulation tasks (such as pushing) in general.
Furthermore most prior approaches are not capable of selecting whether a grasping or pushing (or dragging) strategy is better suited to solve the given task and also do not exhibit a "retrying" behavior for recovering from failure.
Video-prediction based manipulation is more general than many existing methods for robotic manipulation such as grasping specific \cite{lenz2015deep, goldfeder2009data, zeng2017robotic} or pushing specific works \cite{hermans2013learning, salganicoff1993vision} because it does not require any human intervention at training time (i.e. human labels), large databases of 3D objects meshes and does not make any assumptions about object boundaries or the dimensionality of the state space.

Our method uses a video prediction model to predict future frames from autonomously gathered exploration data, and uses this same data to train a registration model that can be used to evaluate the distance to the goal for the predicted frames. We demonstrate that using a cost function derived from this distance substantially improves performance on temporally extended tasks. We further show that, by including a simple grasping ``reflex'' inspired by the palmar reflex in infants, we can effectively learn both non-prehensile and prehensile object relocation skills, allowing the robot to plan to pick up and move objects when necessary. Our experiments show a large improvement in success rates compared to a prior visual MPC method~\cite{sna}.
