\section{Discussion}
In both \cite{hermans2013learning,salganicoff1993vision} pushing of unknown objects is learned from interaction data between the robot and objects. However the models employed in these works rely on hand-engineered input features which can make it hard to scale to complex real-world scenarios. 
It as been shown in \cite{goldfeder2009data} and others that grasps on unseen objects can be found by matching the sensed 3D geometry to a database of precomputed object-grasp pairs. One downside to this approach is that physics, such as mass and friction are generally ignored in this scheme. Furthermore most prior approaches are not capable of selecting whether a grasping or pushing or dragging strategy is better suited to solve the given task and also do not exhibit a "retrying" behavior for recovering from failure.
Video-prediction based manipulation is more general than many existing methods for robotic manipulation such as grasping specific \cite{lenz2015deep, goldfeder2009data, zeng2017robotic} or pushing specific works \cite{hermans2013learning, salganicoff1993vision} because it does not require any human intervention at training time (i.e. human labels), large databases of 3D objects meshes and does not make any assumptions about object boundaries or the dimensionality of the state space.
%Our method uses a video prediction model to predict future frames from autonomously gathered exploration data, and uses this same data to train a registration model that can be used to evaluate the distance to the goal for the predicted frames.
 We demonstrate that using a cost function derived from learned image-to-image registration substantially improves performance on temporally extended tasks. We further show that, by including a simple grasping ``reflex'' inspired by the palmar reflex in infants, we can effectively learn both non-prehensile and prehensile object relocation skills, allowing the robot to plan to pick up and move objects when necessary. Our experiments show a large improvement in success rates compared to a prior visual MPC method~\cite{sna}.
