\section{Discussion}

%Our method uses a video prediction model to predict future frames from autonomously gathered exploration data, and uses this same data to train a registration model that can be used to evaluate the distance to the goal for the predicted frames.
 We demonstrate that using a cost function derived from learned image-to-image registration substantially improves performance on temporally extended object manipulation tasks by closing the control loop. Our experiments show a large improvement in success rates compared to a prior open-loop visual MPC method~\cite{sna}. We further show that, by including a simple grasping ``reflex'' inspired by the palmar reflex in infants, we can efficiently learn both non-prehensile and prehensile object relocation skills in a purely self-supervised way, allowing the robot to plan to pick up and move objects when necessary.  
Interesting challenges for future work are multi-object relocation tasks, which could be addressed by adding an abstract long-term planner on top of the presented framework.
