
\vspace{-0.1cm}
\section{Experiments}
\vspace{-0.2cm}

\begin{wrapfigure}{r}{.35\columnwidth}
\vspace{-0.4in}
\centering
\includegraphics[width=0.35\columnwidth]{images/pushlong_bench_same_range.pdf}
\vspace{-0.9cm}
\caption{\small{Benchmark for pushing with 20 different tasks evaluated on unseen objects. Fraction of runs where final distance (in pixel units of 48x64 image) is lower than threshold. Our method shows a clear gain over OpenCV tracking and predictor propagation.}}
\label{fig:push_bench_long}
\vspace{-0.1in}
\end{wrapfigure}


Our experimental evaluation, conducted using two real Sawyer robotic arms, evaluate the ability of our method to learn both prehensile and non-prehensile object relocation tasks entirely through autonomously collected data and self-supervision. In particular, we aim to answer the following questions: (1) How does our MPC approach with self-supervised goal image registration compare to alternative cost functions, such as off-the-shelf tracking and forward prediction via flow-based models? (2) When the robot can continuously retry a task using goal image registration, how much is the success rate improved for object relocation tasks? (3) Can we learn predictive models that enable both non-prehensile and prehensile object manipulation? In the appendix we also present additional experimental comparisons in a simulated environment.
Videos and visualizations can be found on our supplementary webpage: \url{https://sites.google.com/view/robustness-via-retrying}.

\begin{wrapfigure}{r}{.35\columnwidth}
	\vspace{-0.2in}
	\centering
	\includegraphics[width=0.35\columnwidth]{images/pushshort_bench_plots.pdf}
	\vspace{-0.8cm}
	\caption{\small{Benchmark for short distance pushing.  Fraction of runs where final distance is lower than threshold.}}
	\label{fig:push_bench_short}
%	\vspace{-0.3in}
\end{wrapfigure}

%\subsection{Real-World Experiments}

%We conduct all of our expeirments on a real Sawyer robot arm. 
To train both our prediction and registration models, we collected 20,000 trajectories of pushing motions and 15,000 trajectories with gripper control, where the robot was allowed to randomly move and pick up objects (we use 150 objects for training and 5 objects for testing; see the appendix for details). The data collection process is fully autonomous, requiring human intervention only to replace and change out the objects in front of the robot.
%%SL.06.15: would be nice to have a picture of data collection maybe?
The action space consists of Cartesian movements along the $x$, $y$, and $z$ axes, and changes in the azimuth orientation of the gripper, while the grasping action is triggered by a primitive as specified in Section~\ref{sec:scalingup}. For evaluation, we select novel objects that were never seen during training. The evaluation tasks required the robot to move objects in its environment from a starting state to a goal configuration, and performance is evaluated by measuring the distance between the final object position and goal position. In all experiments, the maximum episode length is 50 time steps.

\vspace{-0.1in}
\paragraph{Pushing with retrying.}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{images/push_correction.pdf}
    \caption{\small{Applying our method to a pushing task. The goal is to push the bottle to the green point. In the first 3 time instants the object behaves unexpectedly, moving down. The tracking then allows the robot to retry, allowing it to eventually bring the object to the goal.}}
    \label{fig:push_retry}
\end{figure}



In the first experiment, we aim to evaluate the performance of different visual MPC cost functions, including our proposed self-supervised registration cost. For this experiment, we disable the gripper control, requiring the robot to push objects to the target. 

We evaluate our method on 20 long-distance and 15 short-distance pushing tasks. For comparisons, we include an ablation where visual-MPC is used with a cost function that computes object locations using the ``multiple instance learning tracker'' (MIL) \cite{babenko2009visual} in OpenCV. 
%Note that our method does not have any prior knowledge of objects -- it is only provided with the position of one designed pixel in the initial and goal images, and must use the learned model to infer that this pixel belongs to an object that can be moved by the robot. 
We also compare to the visual MPC method proposed by \citet{sna},
which does not track the object explicitly, but relies on the flow-based video prediction model to keep track of the designated pixel, which we call ``predictor propagation.'' 
%We also tested a method where the visual MPC cost is calculated by directly evaluating the error between the predicted image and goal image, but we found that this approach was unable to make meaningful progress in moving the object to the target. Instead it resulted in a policy that always moves the arm to the position indicted in the target image, as the arm accounts for the majority of the movable pixel mass in the image. This can be interpreted as undesirable local minimum which is not present in tracking based cost we propose.

\autoref{fig:push_bench_long} illustrates that, on the long-distance benchmark, where the average distance is $30$ cm,
%%CF.9.30: Specify what is meant by "average distance". Do you mean the starting distance between the object position and it's goal position? Also the threshold in Table 1 is based on pixels, but this is specified in centimeters. Can you say here what 30 cm corresponds to in pixels? (at least roughly)
our approach not only outperforms prior work \cite{sna}, but also outperforms the hand-designed object tracker \cite{babenko2009visual}. This is due to the fact that, using our learned registration, the agent is more frequently able to successfully recover from situations where the object behaved differently than was predicted by the model (see \autoref{fig:push_retry}), or the object became occluded temporarily. In contrast, in the short distance benchmark, where the average distance is $15$ cm
%%CF.9.30: see comment above. what is meant by average distance?
all methods perform comparably, as shown in \autoref{fig:push_bench_short}. These results indicate the importance of closed loop control in long-horizon tasks.

\begin{table}
	{\footnotesize
		\begin{center}
			\begin{tabular}{lcc}
				\toprule
%				 & \multicolumn{2}{c}{fraction of successful runs} \\
				& Short & Long \\
				\midrule
			   Visual MPC $+$ predictor propagation  & 83\% & 20\% \\
  			   Visual MPC $+$ OpenCV tracking  & 83\%  & 45\% \\
			   Visual MPC $+$ registration network (Ours)  & 83\% & \textbf{66\%}  \\
				\bottomrule
			\end{tabular}
		\end{center}
	}
	\caption{\footnotesize Success rate for long-distance pushing benchmark with 20 different object/goal configurations and short-distance benchmark with 15 object/goal configurations. Success is defined as bringing the object closer than 15 pixels to the goal, where the complete image has size 48x64.}
	\label{table:res_longd}
	\vspace{-0.35in}
\end{table}

%\begin{table}
%	\setlength{\belowcaptionskip}{-10pt}
%	% \begin{minipage}[t]{0.77\linewidth}
%	%\vspace{0pt}
%	\resizebox{0.98\linewidth}{!}{
%		\begin{tabular}{l|c}
%			Method $\downarrow$    & min $\ell_1$ err & match-step      & min $\ell_1$ err & match-step       & min $\ell_1$ err & match-step \\
%		\end{tabular}
%	}
%	\caption{Bidirectional frame prediction performance on: grasping, pick-and-place, and two-object pushing. Lower min $\ell_1$ err is better. match-step denotes which times are being predicted. It is clear that TAP methods make better predictions than fixed-time prediction at the same time offsets.}
%	\label{tab:results}
%\end{table}


%%SL.06.15: fill in discussion about what it does

\vspace{-0.1in}
\paragraph{Combined prehensile and non-prehensile manipulation.}

\begin{wrapfigure}{r}{.35\columnwidth}
\vspace{-0.3in}
\centering
\hspace{-0.57cm}\includegraphics[width=0.39\columnwidth]{images/grasping_score_cdf.pdf}
\vspace{-0.9cm}
\caption{\small On the real-world grasping benchmark, our registration method is on par with OpenCV tracking.}
\label{fig:grasp_bench}
\vspace{-0.3in}
\end{wrapfigure}

In the setting where the gripper is enabled, the robot needs to decide whether to solve a task by grasping or pushing. Similar to the pushing setting, we perform a benchmark where we define 20 object relocation tasks and measure the final distance between the object and the target at the end of the episode. Interestingly, we observe that in the majority of the cases the agent decides to grasp the object, as can be seen in the supplementary video. \autoref{fig:grasp_bench} shows the results of a benchmark on long-distance relocation tasks, which show that visual MPC combined with our registration method is comparable with the performance of visual MPC combined with OpenCV tracking. 