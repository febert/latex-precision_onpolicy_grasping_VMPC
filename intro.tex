\section{Introduction}

%%SL.04.20: See the paragraph that I drafted in the comment after the abstract -- maybe some of that can be incorporated here?
Human and animals have the ability to learn complex skills (such as manipulating objects) through millions of interactions with their environment during their lifetime. In this work we aim at building a learning system that interacts with the environment autonomously, building a model that allows for predicting future observations based on motor commands and enables the agent to perform closed loop control towards a given goal state. The motivation for closing the loop is that model errors can be tolerated if the system corrects for mistakes and "retries" until it succeeds. 

%%SL.04.20: I would recommend reading through this carefully and noting the key points that we need to touch on in an introduction: https://cs.stanford.edu/people/widom/paper-writing.html


%%SL.04.20: I think this paragraph should be perhaps the fourth paragraph. After one paragraph of motivation, one paragraph about why prior work doesn't solve it/what's the key idea (kind of like an expanded version of the para at the end of abstract), one paragraph on self-directed play, and then this
We demonstrate our method on the task of moving unknown objects in a table-top setting using a robot manipulator. In order to learn high fidelity manipulation skills in a self-supervised fashion, tasks need to be specified in way that allows for precision: Goal images showing the scene in a desired configuration were chosen to specify goals. A method is introduced that learns in a self-supervised fashion how to plan actions to achieve the desired state specified in the goal-image. The method finds correspondences between the current image, the goal-image and the initial image at the start of the episode. A deep convolutional neural network (called registration-network) is trained for the purpose of finding theses correspondences by learning an image-to-image warping function in a self-supervised way: The registration network learns a function that transforms images from arbitrary timesteps along a video to match frames from other timesteps.
The proposed model-predictive control algorithm using the learned registration is compared to a version that uses key-point matching algorithms based on traditional hand-engineered features (SIFT). 

%%SL.04.20: this should come before
While \cite{crashing} and \cite{foresight, sna} show that self-supervised learn allows solving complex tasks collecting data with random actions, the goal states need to lie in the distribution of states encountered during random data collection. How can self-supervised learning be extended to settings where visiting the goal states under a purely random policy is unlikely (such as performing a pick and place task)? We propose three methods for directing the data collection process to areas of the state space that are relevant for the task and compare their effectiveness by comparing the performance of the resulting model-predictive controllers. 

%%SL.04.20: can we get away without the demonstrations, or have we already concluded that this is hopeless? I think the self-supervised story comes together much better if we can avoid the demonstrations.
A small number of demonstrations (for example using kinesthetic teaching or teleoperation) can be used to train a policy by behavioural cloning. Although the imitation policy only achieves a very low success rate, it still allows visiting states that need to be visited to solve the task. In this work we consider the setting of learning how to pick and place objects in a tabletop environment. While a purely random policy would only rarely pick up objects, the imitation model ensures that successful grasps occur at a certain fraction of the time, therefore allowing the prediction model to accurately model the contact dynanmics encountered during grasping. To ensure additional exploration and avoid the dynamics model learning a non-causal model\footnote{If the success rate during datacollection is too high the forward prediction model becomes "optimistic" and would falsely predict successful trajectory while ignoring the action information.} noise is added to the imitation policy during data collection. Using this imitaton model and initial datset is collected that is used to train the forward model (i.e. the video-prediction model) used in MPC.

After a model-predictive control algorithm has been learned, additional data is collected under the current MPC policy for a specific automatically assigned goal. The data collected under the MPC-policy enables improving the prediction model precisely at the states frequently visited by the MPC-controller. Since the MPC-policy changes as the on-policy data is added to the initial dataset the prediction model needs to be retrained. Therefore we propose an iterative data collection and model-training process simillar to  \cite{anusha}
