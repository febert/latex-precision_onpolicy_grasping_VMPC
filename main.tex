\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission

\usepackage[]{biblatex}
\addbibresource{bib.bib}

\usepackage[nonatbib]{nips_2018}


% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[svgnames]{xcolor}

\providecommand\comment[1]{\textbf{COMMENT:} \textcolor{Magenta}{#1}}

\title{Precision from Retrying: Closed-Loop Robotic Manipulation using Self-Supervised Learning}
%%SL.04.20: Let's think about the title a bit more. This seems like a reasonable starting point. But first let's list the main ideas that seem to make the work exciting:
%The ability to fix mistakes
%The ability to reach goal images by comparing them directly to the current image
%The ability to achieve goals to high accuracy
%The ability to do on-policy data collection to acquire more complex skills like grasping
% Some things are exciting, but not inherently new to this work, including: self-supervised, closed-loop, etc.
%  on the other hand, instead of the somewhat generic 'self-supervised learning,' we can probably come up with something
%  a bit more descriptive: one thing that makes our approach different is that, with on-policy data collection, the robot
%  is essentially setting its own goals and then attempting to reach them, so we can call it something like 'self-directed play'
% Some things are exciting but not mentioned, including the fact that this is done with raw images...
% Here are a few thoughts I have about possible titles:
%Visual Learning with Self-Directed Play: Acquisition of Complex Robotic Skills with Direct Video Prediction
%Self-Directed Play with Visual Recall: Learning Manipulation Tasks by Reaching Previous Observations
%Learning Vision-Based Robotic Skills with Self-Directed Play
%...
% Another direction we could go is to emphasize that the method can succeed even with an imperfect predictor, by trying again, but I'm not really sure how to fit that into a very concise title
%%SL.04.20: it would also be good to get some illustrated diagram put together soon to explain how the method works... I feel like a diagram might be quite important here

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Self-supervised learning methods, in particular video-prediction has recently been successfully used in robotic manipulation showing that generalizable skills can arise without the need for any external supervision.
%%SL.04.20: It would be a good idea to start with one sentence of fairly broad motivation to explain the bigger context of why the problem matters. It doesn't really need to be specific to video prediction, but more of a broad motivation for self-supervised visual learning of skills.
Self-supervised methods have the advantage that data is collected automatically in large amounts while removing the need for expensive data labeling. However the manipulation skills obtained in this way thus far have been coarse.
%%SL.04.20: not obvious what coarse means
The first contribution of this work is a video-prediction based model-predictive control (MPC)
%%SL.04.20: I'm inclined to suggest leaving the jargon (like MPC) out of the abstract for now, and using more accessible phrasing.
algorithm that achieves high precision by incorporating a learned image-to-image registration module that allows a user to specify target states precisely using goal-images. The model-predictive control algorithm continuously registers the current observations to the goal image allowing it to correct for prediction errors by "retrying" until the goal is achieved. 

Past self-supervised learning approaches collect data by applying random actions thus restricting range of states where the learned model is valid. The second contribution is a method for directing the data collection towards task-relevant areas of the state space. We propose a method that leverages small numbers of demonstrations and collects additional on-policy data by running the proposed model-predictive control algorithm with an automatically selected goal. In addition to using demonstrations our method also allows for making use of prior domain knowledge to further improve the data collection process: Data collection can be started with simple hand-engineered policies that only requires a very low success rates.
\end{abstract}
%%SL.04.20: Here is my attempt at an alternative abstract that I thing brings out some of the more exciting things about the work. Maybe this can serve as some inspiration for how to get a more focused abstract in place that at the same time still touches on the big picture stuff:
%Understanding how the world works -- which actions lead to which future events -- can allow humans and animals to act intelligently in complex and unfamiliar situations. Crucially, understanding and predicting physical interactions does not require any supervision beyond observation of one's own actions and their consequences. This makes prediction an appealing objective for self-supervised learning of behavioral skills, for example for autonomous robots. However, incorporating prediction of future sensory inputs into an end-to-end framework for decision making and control poses a number of major challenges. How should the data for learning to predict be collected? How should the predictive model be used? What happens when the predictions are inaccurate? In this paper, we tackle these questions by proposing a method for learning complex robotic skills from raw image observations, using only autonomously collected experience. Our method is based on two key ideas. First, we use self-directed play to collect data, where the robot autonomously proposes goals and then attempts to execute them. We demonstrate in simulation that this substantially improves performance over random data collection. Second, we observe that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). By incorporating end-to-end learned registration into our method, we can enable the robot to continuously retry the task until it gets it right. We demonstrate that these two ideas can be combined to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping and repositioning objects, non-prehensile pushing, and maneuvering objects around obstacles. Our real-world experiments demonstrate a model trained with ??? hours of autonomously collected experience with self-directed play, and testing on previously unseen objects.

\input{intro}


\printbibliography
\end{document}
