\documentclass{article}

% \usepackage{corl_2018}
\usepackage[final]{corl_2018} % Uncomment for the camera-ready ``final'' version
\usepackage{wrapfig}

\usepackage{subcaption}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

\providecommand\todo[1]{\textcolor{Red}{#1}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[svgnames]{xcolor}
\usepackage{authblk}

\providecommand\comment[1]{\textbf{COMMENT:} \textcolor{Magenta}{#1}}

\setlength{\bibsep}{4pt plus 0.3ex}

% \title{Precision via Persistence: Closed-Loop Robotic Manipulation with Self-Supervised Learning}
\title{Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning}
% How about this title? We're actually getting more robustness than precision from the tracking..
% Hmm. Maybe Robustness via Retrying? Though, I like Persistence.

%%SL.06.12: I really like the title! Especially the first part. I do not think it's regrettable though that it doesn't really say much about how the method works, but maybe that's OK? It's definitely very catchy and manages to capture the main *new* contribution quite well. Maybe you can work in the word "visual" or "vision-based" before closed-loop? that makes it much more impressive -- lots of things are closed-loop, but not many things close the loop on vision
%%CF.06.11: How about the following: Precision via Persistence: Closed-Loop Robotic Manipulation with Self-Supervised Learning
%%SL.04.20: Let's think about the title a bit more. This seems like a reasonable starting point. But first let's list the main ideas that seem to make the work exciting:
%The ability to fix mistakes
%The ability to reach goal images by comparing them directly to the current image
%The ability to achieve goals to high accuracy
%The ability to do on-policy data collection to acquire more complex skills like grasping
% Some things are exciting, but not inherently new to this work, including: self-supervised, closed-loop, etc.
%  on the other hand, instead of the somewhat generic 'self-supervised learning,' we can probably come up with something
%  a bit more descriptive: one thing that makes our approach different is that, with on-policy data collection, the robot
%  is essentially setting its own goals and then attempting to reach them, so we can call it something like 'self-directed play'
% Some things are exciting but not mentioned, including the fact that this is done with raw images...
% Here are a few thoughts I have about possible titles:
%Visual Learning with Self-Directed Play: Acquisition of Complex Robotic Skills with Direct Video Prediction
%Self-Directed Play with Visual Recall: Learning Manipulation Tasks by Reaching Previous Observations
%Learning Vision-Based Robotic Skills with Self-Directed Play
%...
% Another direction we could go is to emphasize that the method can succeed even with an imperfect predictor, by trying again, but I'm not really sure how to fit that into a very concise title
%%SL.04.20: it would also be good to get some illustrated diagram put together soon to explain how the method works... I feel like a diagram might be quite important here

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

% NOTE: authors will be visible only in the camera-ready (ie, when using the option 'final'). 
% 	For the initial submission the authors will be anonymized.

% \author{
%   David S.~Hippocampus\\
%   Department of Electrical Engineering and Computer Sciences\\
%   University of California Berkeley 
%   United States\\
%   \texttt{hippo@berkeley.edu} \\
%   %% examples of more authors
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \AND
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
% }


\author[1]{Frederik Ebert}
\author[1]{Sudeep Dasari}
\author[1]{Chelsea Finn}
\author[1]{Alex X. Lee}
\author[1]{Sergey Levine}

\affil[1]{\footnotesize Department of Electrical Engineering and Computer Sciences, UC Berkeley, United States}

\affil[ ]{\texttt{\{febert, sdasari, cbfinn,alexlee\_gk,svlevine\}@berkeley.edu}}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
% Understanding how the world works -- which actions lead to which future events -- can allow humans to act intelligently in complex and unfamiliar situations. Crucially, understanding and predicting physical interactions does not require any supervision beyond observation of one's own actions and their consequences. This makes
Prediction is an appealing objective for self-supervised learning of behavioral skills in particular for autonomous robots. However, incorporating prediction of future sensory inputs into an end-to-end framework for decision making and control poses a number of major challenges. How should the predictive model be used? What happens when the predictions are inaccurate? In this paper, we tackle these questions by proposing a method for learning complex robotic skills from raw image observations, using only autonomously collected experience. We show that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). By incorporating  learned registration into our method, we can enable a robot to continuously retry the task until it gets it right. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation. Our real-world experiments demonstrate a model trained with 160 robot hours of autonomously collected data is able to successfully master complex manipulation tasks with a wide range of objects not seen during training. 

\end{abstract}
%%SL.04.20: Here is my attempt at an alternative abstract that I think brings out some of the more exciting things about the work. Maybe this can serve as some inspiration for how to get a more focused abstract in place that at the same time still touches on the big picture stuff:
%Understanding how the world works -- which actions lead to which future events -- can allow humans and animals to act intelligently in complex and unfamiliar situations. Crucially, understanding and predicting physical interactions does not require any supervision beyond observation of one's own actions and their consequences. This makes prediction an appealing objective for self-supervised learning of behavioral skills, for example for autonomous robots. However, incorporating prediction of future sensory inputs into an end-to-end framework for decision making and control poses a number of major challenges. How should the data for learning to predict be collected? How should the predictive model be used? What happens when the predictions are inaccurate? In this paper, we tackle these questions by proposing a method for learning complex robotic skills from raw image observations, using only autonomously collected experience. Our method is based on two key ideas. First, we use self-directed play to collect data, where the robot autonomously proposes goals and then attempts to execute them. We demonstrate in simulation that this substantially improves performance over random data collection. Second, we observe that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). By incorporating end-to-end learned registration into our method, we can enable the robot to continuously retry the task until it gets it right. We demonstrate that these two ideas can be combined to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping and repositioning objects, non-prehensile pushing, and maneuvering objects around obstacles. Our real-world experiments demonstrate a model trained with ??? hours of autonomously collected experience with self-directed play, and testing on previously unseen objects.

\input{intro}

\input{relatedwork}

\input{preliminaries.tex}


\input{registration}

\input{planning_costs.tex}

\input{trainining_procedure.tex}

\input{multi-viewcontrol.tex}



\input{experiments}

\input{conclusion.tex}

%===============================================================================

% The maximum paper length is 8 pages excluding references and acknowledgements, and 10 pages including references and acknowledgements

%\clearpage
% The acknowledgments are automatically included only in the final version of the paper.
%\acknowledgments{If a paper is accepted, the final camera-ready version will (and probably should) include acknowledgments. All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support.}

%===============================================================================

% no \bibliographystyle is required, since the corl style is automatically used.
\bibliography{bib}  % .bib


\newpage
\appendix

{\footnotesize
\input{appendix.tex}
}

\end{document}
