\vspace{-0.1cm}
\section{Preliminaries}
\label{sec:prelim}
\vspace{-0.2cm}

Our visual MPC problem formulation follows the problem statement outlined in prior work~\cite{foresight}. In this setting, an action-conditioned video prediction model $g$, typically represented by a deep neural network, is used to predict future camera observations $\hat{I}_{1:T} \in \mathbb{R}^{T \times H\times W \times 3}$, conditioned on a sequence of candidate actions $a_{1:T}$, where the prediction horizon is $T$. This can be written as $\hat{I}_{1:T} = g(a_{1:T}, I_0)$, where $I_0$ is the frame from the current time-step. An optimization-based planner is  used to select the action sequence that results in an outcome that accomplishes a user-specified goal. This type of vision-based control is highly general, since it reasons over raw pixel observations without the requirement for a fixed-size state space, and has been demonstrated to generalize effectively to non-prehensile manipulation of previously unseen objects~\cite{foresight,sna}.

Visual MPC assumes that the task can be defined in terms of pixel motion. Formally, in the initial image $I_0$ we define $n$ source pixel locations denoted by the coordinates $d_{0,i} \in \mathbb{N}^2$ (for $i \in [0,..n]$) and the analogous for the goal image $I_g$ denoted by $d_{g,i} \in \mathbb{N}^2$. Given a goal, visual MPC plans for a sequence of actions $a_{1:T}$
%%CF.9.29: Do you intend to use the same notation for prediction horizon and planning horizon? FE: Yes
to move the pixel at $d_{0,i}$ to $d_{g,i}$. If this pixel lies on top of an object, this corresponds to moving that entire object to a goal position. Note that this problem formulation resembles visual servoing, but it is considerably more complex, since moving the object at $d_0$ might require complex non-prehensile or prehensile manipulation and long-horizon reasoning.
The planning problem is formulated as the minimization of a cost function $c$, which in accordance with prior work \cite{sna}, measures the distance between the predicted pixel positions $\hat{d}_{\tau}$ and the goal position $d_g$ for each pixel $i$:
\begin{align}
c = \sum^n_{i = 1}  \lambda_i c_i && c_i = \sum_{\tau = 1, \dots, T} \mathbb{E}_{\hat{d}_{\tau,i} \sim P_{\tau,i}} \left[\|\hat{d}_{\tau,i} - d_{g,i}\|_2\right]  
\label{eq:cost}
\end{align}
where $c_i\in \mathbb{R}$ are the costs per source pixel, $\lambda_i$ are weighting factors discussed in section \ref{sec:reg} and $P_{\tau,i}$ is the distribution over predicted pixel positions. The advantage of this distance-based cost functions is that they are well-shaped and can be optimized efficiently. 

In this paper we use the video prediction model architecture developed by~\cite{savp}, where future images are generated by transforming past images. Starting with a distribution over initial positions of the designated pixel \mbox{$P_{t_0,i}\in\mathbb{R}^{H\times W}, \sum_{H,W} P_{t_0,i} = 1$} at time $t = 0$, the model predicts distributions over its positions $P_{t,i}$ at time $t \in \{ 1, \dots, T \}$ by exploiting the image transformations used to generate future frames. Planning is performed by sampling candidate actions sequences and optimizing using the cross-entropy method (CEM) \cite{cem-rk-13} to achieve the lowest possible cost $c$.

To obtain the best results with imperfect models, the action sequence is replanned at each real-world time step\footnote{we refer to time-steps happening in the realworld as $t$ and to predicted time-steps as $\tau$} $t \in \{0,...,t_{max}\}$ following the framework of model-predictive control (MPC): at each real-world step $t$, the first action of the best action sequence is executed. 
At the first real-world time step $t=0$, the distribution $P_{\tau=0,i}$ is initialized as 1 at the location of the designated pixel and zero elsewhere. In prior work \cite{sna, foresight}, in subsequent steps ($t > 0$),  the prediction of the previous step is used to initialize $P_{\tau=0,i}$. However this causes accumulating errors, often preventing the model from solving long-term tasks or responding to situations where the environment (e.g. objects) behaved differently than expected. In effect, the model loses track of which object was designated in the initial image.






