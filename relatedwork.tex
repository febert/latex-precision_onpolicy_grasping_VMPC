\vspace{-0.1cm}
\section{Related Work}
\vspace{-0.1cm}
In prior work \cite{hermans2013learning,salganicoff1993vision}, pushing of unknown objects is learned from interaction data between the robot and objects. However the models employed there rely on hand-engineered input features which can make it hard to scale to complex real-world scenarios. 
It as been shown in \cite{goldfeder2009data} and others that grasps on unseen objects can be found by matching the sensed 3D geometry to a database of precomputed object-grasp pairs, which has the downside that physical properties such as mass and friction are ignored. Furthermore most prior approaches are not capable of selecting whether a grasping, pushing or dragging strategy is better suited to solve the given task and also do not exhibit a "retrying" behavior for recovering from failure.
Video-prediction based manipulation is more general than many existing methods for robotic manipulation such as grasping specific \cite{lenz2015deep, goldfeder2009data, zeng2017robotic} or pushing specific works \cite{hermans2013learning, salganicoff1993vision} because it does not require any human intervention at training time, since it is fully self-supervised. Also it does not make any assumptions about the dimensionality of the state space.

Many self-supervised robot learning methods have focused on predicting the outcome of a particular event such as grasp success~\cite{lerrel,google_handeye,princeton_pushgrasp} or crashing~\cite{crashing,greg_kahn_uncertainty}. Consequently, these approaches can only recover a single policy that optimizes the probability of the predicted event. Instead, we aim to acquire a model that can be reused for multiple goals and behaviors. To do so, we build upon prior works that learn to predict the sequence of future observations, which can be used to optimize with respect to a variety of goals~\cite{foresight,sna,se3_control}. Unlike~\cite{se3_control}, we demonstrate complex object manipulations with previously-unseen objects from RGB video. In contrast to prior self-supervised visual planning works~\cite{foresight,sna}, we can perform substantially longer tasks, by using image registration with respect to a goal image.

Goal observations have been previously used for specifying a reward function for robot learning systems~\cite{jagersand1995visual,deguchi1999image,e2c,dsae}. Unlike these methods, we use a learned registration to the goal image for measuring distance to the goal rather than distance in a learned feature space. Distances in unconstrained feature spaces can be arbitrary, while registration inherently measures how pixels should move and can therefore provide a calibrated distance metric with respect to the goal. 

A related problem is visual servoing, where visual feedback is used to reach a goal configuration~\cite{hutchinson1996tutorial,kragic2002survey,desouza2002survey}.
Traditional approaches aim to minimize distances of feature points~\cite{feddema1989vision,espiau1992servo,wilson1996relative}, or pixel intensities~\cite{caron2013photometric}. Learning and convolutional networks have also been incorporated into visual servoing~\cite{saxena2017servoing,bateux2018servoing,lee2017servoing,google_handeye}. Unlike servoing approaches that use reactive control laws, we use multi-step predictive models to achieve temporally extended goals, while still using continuous visual feedback for retrying at every step. Further, our method performs non-prehensile manipulation, while visual servoing typically assumes fully actuated control, often with a known Jacobian.

Model-predictive control (MPC)~\cite{camacho2013model} has proven successful in a variety of robotic tasks~\cite{shim2003decentralized,allibert2010predictive,howard2010receding,williams2017information,deep_mpc}.
MPC involves planning with a short horizon and re-planning as the execution progresses, providing the foundation of persistent retrying and the ability to overcome inaccuracies in the predictive model. However, as we later show, maintaining an accurate model of the \emph{cost} used for planning throughout an episode is critical, and has prevented prior work on visual foresight~\cite{foresight,sna} from moving beyond short-term tasks.
Our primary contributions is a grounded mechanism for evaluating the planning cost of visual predictions, allowing persistent re-planning with video prediction models.

It possible to use off-the-shelf trackers~\cite{lucas1981iterative,brox2004high,babenko2009visual,mei2009robust} to help address this issue. However, these trackers usually only have a limited capability to adapt to the domain they are applied to, and can lose track during occlusions. A key advantage of our learned registration approach, inspired by \cite{meister2017unflow}, is that we can obtain registration success metrics for every point in the image and every pair of images and therefore we can propose a weighting scheme for the planning costs according to the \emph{uncertainty} of the individual registrations. Furthermore, since our method is completely self-supervised, it continues to improve as more data is collected by the robot.
