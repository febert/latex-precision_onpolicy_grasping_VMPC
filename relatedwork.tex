\section{Related Work}

% CF: cite Visual Robot Task Planning Chris Paxton somewhere -- CF: this actually trains using demonstrations... so maybe not as relevant as we thought.

Many self-supervised robot learning methods have focused on predicting the outcome of a particular event such as grasp success~\cite{lerrel,google_handeye,princeton_pushgrasp} or crashing~\cite{crashing,greg_kahn_uncertainty}. Consequently, these approaches can only recover a single policy that optimizes the probability of the predicted event. Instead, we aim to acquire a model that can be reused for multiple goals and behaviors. To do so, we build upon prior works that learn to predict the sequence of future observations, which can be used to optimize with respect to a variety of goals~\cite{foresight,sna,se3_control}. Unlike~\citet{se3_control}, we demonstrate complex object manipulations with previously-unseen objects from RGB video. In contrast to prior self-supervised visual planning works~\cite{foresight,sna}, we can perform substantially longer tasks, by using image registration with respect to a goal image.
%%CF: a more technical discussion of the relation to SE-3 pose nets would be nice. 
%%CF: add transition from previous to next paragraph. shouldn't be too hard.
%One of our key contributions is the ability to plan with video prediction models with respect to a goal image. 
Goal observations have been previously used for specifying a reward function for robot learning systems~\cite{jagersand1995visual,deguchi1999image,e2c,dsae}. Unlike these methods, we use a learned registration to the goal image for measuring distance to the goal rather than distance in a learned feature space. Distances in unconstrained feature spaces can be arbitrary, while registration inherently measures how pixels should move and can therefore provide a calibrated distance metric with respect to the goal.


%to indicate the goal to a policy in multitask reinforcement learning settings~\cite{uvfa,stanford_navigation} and


%TODO: reconcile the above and below paragraph.

Another related robotics problem is visual servoing, where visual feedback is used to control a robot to reach a goal configuration~\cite{hutchinson1996tutorial,kragic2002survey,desouza2002survey}.
Traditional approaches aim to minimize distances of feature points~\cite{feddema1989vision,espiau1992servo,wilson1996relative}, or pixel intensities~\cite{caron2013photometric}. Learning and convolutional networks have also been incorporated into visual servoing~\cite{saxena2017servoing,bateux2018servoing,lee2017servoing,google_handeye}. Unlike servoing approaches that use reactive control laws, we use multi-step predictive models to achieve temporally extended goals, while still using continuous visual feedback for retrying at every step. Furthermore, our method performs both prehensile and non-prehensile manipulation, while visual servoing typically assumes fully actuated control, often with a known Jacobian.

%Paragraph on robot retrying and correcting for mistakes using MPC.

Model-predictive control (MPC)~\cite{camacho2013model} has proven successful in a variety of robotic tasks~\cite{shim2003decentralized,allibert2010predictive,howard2010receding,williams2017information,deep_mpc}.
MPC involves planning for controls with a short horizon and re-planning as the execution progresses.
Thus, it provides the foundation of persistent retrying and the ability to overcome inaccuracies of the predictive model. However, as we later show, maintaining an accurate model of the \emph{cost} used for planning throughout an episode is critical for success, and has prevented prior work on visual foresight~\cite{foresight,sna} from moving beyond short-term tasks.
One of our primary contributions is a grounded mechanism for evaluating the planning cost of visual predictions, allowing persistent re-planning with video prediction models which enables longer-term manipulations.

%
%In contrast to the short-horizon pushing skills demonstrated in prior work~\cite{}, we show that our video prediction model can be used to perform longer-term manipulations and automatically choose to push and pick objects, acquiring rudimentary grasping behaviors entirely via video prediction.

%this registration network learns a function that transforms images from arbitrary timesteps along a video to match frames from other timesteps. 
%Discuss methods for image registration (both supervised and unsupervised), and how ours compares / contrasts
%%SL.06.12: maybe put this a bit later in the section, not too early


It is also possible to use off-the-shelf trackers~\cite{lucas1981iterative,brox2004high,babenko2009visual,mei2009robust} to address this issue. However, these trackers usually only have a limited capability to adapt to the domain they are applied to, and can lose track during occlusions. A key advantage of our learned registration approach, inspired by \cite{meister2017unflow}, is that we can obtain registration success metrics for every point in the image and every pair of images and therefore we can propose a weighting scheme for the planning costs according to the \emph{uncertainty} of the individual registrations. Furthermore, since our method is completely self-supervised, it continues to improve as more data is collected by the robot.


%\begin{itemize}
%    \item paragraph on grasping papers? (or cite survey)
%    %%SL.06.12: good to cite (esp if we get a grasping person), but make it very clear how our goals are fundamentally different -- we don't want people to think that our method should be compared to grasping methods
%\end{itemize}